{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5960, 13)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/hmeq.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "1. [Drop empty rows](#Drop-empty-rows)\n",
    "2. [Get dummies](#Get-dummies)\n",
    "3. [Drop correlated](#Drop-correlated)\n",
    "4. [Train test split](#Train-test-split)\n",
    "5. [Imputing](#Imputing)\n",
    "6. [Unskewing](#Unskewing)\n",
    "7. [Scale data](#scale-data)\n",
    "8. [Balance data](#balance-data)\n",
    "9. [Saving cleaned data](#Saving-cleaned-datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop empty rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data analysis showed that rows 3 and 1405 where completely empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      BAD   LOAN  MORTDUE  VALUE REASON  JOB  YOJ  DEROG  DELINQ  CLAGE  NINQ  \\\n",
      "3       1   1500      NaN    NaN    NaN  NaN  NaN    NaN     NaN    NaN   NaN   \n",
      "1405    0  10800      NaN    NaN    NaN  NaN  NaN    NaN     NaN    NaN   NaN   \n",
      "\n",
      "      CLNO  DEBTINC  \n",
      "3      NaN      NaN  \n",
      "1405   NaN      NaN  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BAD</th>\n",
       "      <th>LOAN</th>\n",
       "      <th>MORTDUE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>REASON</th>\n",
       "      <th>JOB</th>\n",
       "      <th>YOJ</th>\n",
       "      <th>DEROG</th>\n",
       "      <th>DELINQ</th>\n",
       "      <th>CLAGE</th>\n",
       "      <th>NINQ</th>\n",
       "      <th>CLNO</th>\n",
       "      <th>DEBTINC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1700</td>\n",
       "      <td>97800.0</td>\n",
       "      <td>112000.0</td>\n",
       "      <td>HomeImp</td>\n",
       "      <td>Office</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>1</td>\n",
       "      <td>10800</td>\n",
       "      <td>52600.0</td>\n",
       "      <td>66700.0</td>\n",
       "      <td>DebtCon</td>\n",
       "      <td>ProfExe</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>110.566667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      BAD   LOAN  MORTDUE     VALUE   REASON      JOB  YOJ  DEROG  DELINQ  \\\n",
       "4       0   1700  97800.0  112000.0  HomeImp   Office  3.0    0.0     0.0   \n",
       "1407    1  10800  52600.0   66700.0  DebtCon  ProfExe  2.0    0.0     0.0   \n",
       "\n",
       "           CLAGE  NINQ  CLNO  DEBTINC  \n",
       "4      93.333333   0.0  14.0      NaN  \n",
       "1407  110.566667   5.0  26.0      NaN  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_empty_rows_df = df.copy()\n",
    "print(no_empty_rows_df.iloc[[3, 1405]])\n",
    "no_empty_rows_df.drop([3, 1405], axis=0, inplace=True)\n",
    "no_empty_rows_df.iloc[[3, 1405]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data analysis we saw that JOB and REASON where categorical features so I'll first make dummy variables for these. I'll also add a new category \"missing\" to be used as an imputation for the null values so we don't lose the information of them being null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# missing values JOB: 277\n",
      "# missing values JOB: 0\n",
      "# missing values REASON: 250\n",
      "# missing values REASON: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BAD</th>\n",
       "      <th>LOAN</th>\n",
       "      <th>MORTDUE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>YOJ</th>\n",
       "      <th>DEROG</th>\n",
       "      <th>DELINQ</th>\n",
       "      <th>CLAGE</th>\n",
       "      <th>NINQ</th>\n",
       "      <th>CLNO</th>\n",
       "      <th>...</th>\n",
       "      <th>JOB_Mgr</th>\n",
       "      <th>JOB_Office</th>\n",
       "      <th>JOB_Other</th>\n",
       "      <th>JOB_ProfExe</th>\n",
       "      <th>JOB_Sales</th>\n",
       "      <th>JOB_Self</th>\n",
       "      <th>JOB_missing</th>\n",
       "      <th>REASON_DebtCon</th>\n",
       "      <th>REASON_HomeImp</th>\n",
       "      <th>REASON_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1100</td>\n",
       "      <td>25860.0</td>\n",
       "      <td>39025.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.366667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1300</td>\n",
       "      <td>70053.0</td>\n",
       "      <td>68400.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>121.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>16700.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>149.466667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1700</td>\n",
       "      <td>97800.0</td>\n",
       "      <td>112000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>30548.0</td>\n",
       "      <td>40320.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.466002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BAD  LOAN  MORTDUE     VALUE   YOJ  DEROG  DELINQ       CLAGE  NINQ  CLNO  \\\n",
       "0    1  1100  25860.0   39025.0  10.5    0.0     0.0   94.366667   1.0   9.0   \n",
       "1    1  1300  70053.0   68400.0   7.0    0.0     2.0  121.833333   0.0  14.0   \n",
       "2    1  1500  13500.0   16700.0   4.0    0.0     0.0  149.466667   1.0  10.0   \n",
       "4    0  1700  97800.0  112000.0   3.0    0.0     0.0   93.333333   0.0  14.0   \n",
       "5    1  1700  30548.0   40320.0   9.0    0.0     0.0  101.466002   1.0   8.0   \n",
       "\n",
       "   ...  JOB_Mgr  JOB_Office  JOB_Other  JOB_ProfExe  JOB_Sales  JOB_Self  \\\n",
       "0  ...    False       False       True        False      False     False   \n",
       "1  ...    False       False       True        False      False     False   \n",
       "2  ...    False       False       True        False      False     False   \n",
       "4  ...    False        True      False        False      False     False   \n",
       "5  ...    False       False       True        False      False     False   \n",
       "\n",
       "   JOB_missing  REASON_DebtCon  REASON_HomeImp  REASON_missing  \n",
       "0        False           False            True           False  \n",
       "1        False           False            True           False  \n",
       "2        False           False            True           False  \n",
       "4        False           False            True           False  \n",
       "5        False           False            True           False  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_var_df = no_empty_rows_df.copy()\n",
    "\n",
    "print(f\"# missing values JOB: {dummy_var_df['JOB'].isnull().sum()}\")\n",
    "dummy_var_df['JOB'] = dummy_var_df['JOB'].fillna(\"missing\")\n",
    "print(f\"# missing values JOB: {dummy_var_df['JOB'].isnull().sum()}\")\n",
    "\n",
    "print(f\"# missing values REASON: {dummy_var_df['REASON'].isnull().sum()}\")\n",
    "dummy_var_df['REASON'] = dummy_var_df['REASON'].fillna(\"missing\")\n",
    "print(f\"# missing values REASON: {dummy_var_df['REASON'].isnull().sum()}\")\n",
    "\n",
    "job_dummies = pd.get_dummies(dummy_var_df['JOB'], prefix=\"JOB\")\n",
    "reason_dummies = pd.get_dummies(dummy_var_df['REASON'], prefix=\"REASON\")\n",
    "\n",
    "dummy_var_df = pd.concat([dummy_var_df, job_dummies, reason_dummies], axis=1)\n",
    "dummy_var_df.drop([\"JOB\", \"REASON\"], axis=1, inplace=True)\n",
    "dummy_var_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MORTDUE and VALUE had very high correlation (88%), meaning that they produce a lot of redundant information so I'll drop MORTDUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BAD', 'LOAN', 'VALUE', 'YOJ', 'DEROG', 'DELINQ', 'CLAGE', 'NINQ',\n",
       "       'CLNO', 'DEBTINC', 'JOB_Mgr', 'JOB_Office', 'JOB_Other', 'JOB_ProfExe',\n",
       "       'JOB_Sales', 'JOB_Self', 'JOB_missing', 'REASON_DebtCon',\n",
       "       'REASON_HomeImp', 'REASON_missing'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_mortdue_df = dummy_var_df.copy()\n",
    "\n",
    "drop_mortdue_df.drop(\"MORTDUE\", axis=1, inplace=True)\n",
    "drop_mortdue_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can balance, impute, scale and unskew our data we need to first split it in a train and test set to prevent information leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num of instances: 5958, total num of features: 20\n",
      "num of ind feature instances: 5958, num of ind feature features: 19\n",
      "num of dep feature instances: 5958\n",
      "num of training instances: 4766, num of training features: 19\n",
      "num of test instances: 1192, num of test features: 19\n",
      "num of train target instances: 4766\n",
      "num of test target: 1192\n"
     ]
    }
   ],
   "source": [
    "X = drop_mortdue_df.drop(\"BAD\", axis=1)\n",
    "y = drop_mortdue_df[\"BAD\"]\n",
    "\n",
    "print(f\"Total num of instances: {drop_mortdue_df.shape[0]}, total num of features: {drop_mortdue_df.shape[1]}\")\n",
    "print(f\"num of ind feature instances: {X.shape[0]}, num of ind feature features: {X.shape[1]}\")\n",
    "print(f\"num of dep feature instances: {y.shape[0]}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "print(f\"num of training instances: {X_train.shape[0]}, num of training features: {X.shape[1]}\")\n",
    "print(f\"num of test instances: {X_test.shape[0]}, num of test features: {X.shape[1]}\")\n",
    "print(f\"num of train target instances: {y_train.shape[0]}\")\n",
    "print(f\"num of test target: {y_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_X_train = X_train.copy()\n",
    "imputed_X_test = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          CLAGE  NINQ  CLNO\n",
      "4758  10.133333  41.0  41.0\n",
      "1091   0.507115   8.0   8.0\n",
      "1122   3.044384   8.0   8.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLAGE</th>\n",
       "      <th>NINQ</th>\n",
       "      <th>CLNO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>9.100000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>11.963733</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>9.534143</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>4.412770</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>2.820786</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>5.243341</td>\n",
       "      <td>41.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          CLAGE  NINQ  CLNO\n",
       "691    9.100000   8.0   8.0\n",
       "1360  11.963733   7.0   7.0\n",
       "1347   9.534143   7.0   7.0\n",
       "1572   4.412770   8.0   8.0\n",
       "1083   2.820786   7.0   7.0\n",
       "4845   5.243341  41.0  41.0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set credt lines that are younger then 12months as recent\n",
    "train_indices = imputed_X_train[(imputed_X_train[\"NINQ\"].isnull()) & -(imputed_X_train[\"CLNO\"].isnull()) & (imputed_X_train[\"CLAGE\"] < 12)][[\"CLAGE\", \"NINQ\", \"CLNO\"]].index\n",
    "imputed_X_train.loc[train_indices, \"NINQ\"] = imputed_X_train.loc[train_indices, \"CLNO\"]\n",
    "\n",
    "test_indices = imputed_X_test[(imputed_X_test[\"NINQ\"].isnull()) & -(imputed_X_test[\"CLNO\"].isnull()) & (imputed_X_test[\"CLAGE\"] < 12)][[\"CLAGE\", \"NINQ\", \"CLNO\"]].index\n",
    "imputed_X_test.loc[test_indices, \"NINQ\"] = imputed_X_test.loc[test_indices, \"CLNO\"]\n",
    "\n",
    "print(imputed_X_test.loc[test_indices, [\"CLAGE\", \"NINQ\", \"CLNO\"]])\n",
    "imputed_X_train.loc[train_indices, [\"CLAGE\", \"NINQ\", \"CLNO\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(series):\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    return imp.fit_transform(series.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LOAN              0\n",
       "VALUE             0\n",
       "YOJ               0\n",
       "DEROG             0\n",
       "DELINQ            0\n",
       "CLAGE             0\n",
       "NINQ              0\n",
       "CLNO              0\n",
       "DEBTINC           0\n",
       "JOB_Mgr           0\n",
       "JOB_Office        0\n",
       "JOB_Other         0\n",
       "JOB_ProfExe       0\n",
       "JOB_Sales         0\n",
       "JOB_Self          0\n",
       "JOB_missing       0\n",
       "REASON_DebtCon    0\n",
       "REASON_HomeImp    0\n",
       "REASON_missing    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"YOJ\", \"VALUE\", \"DEROG\", \"DELINQ\", \"CLAGE\", \"NINQ\", \"CLNO\", \"DEBTINC\"]\n",
    "\n",
    "for col in columns:\n",
    "    imputed_X_train[col] = impute(imputed_X_train[col])\n",
    "    imputed_X_test[col] = impute(imputed_X_test[col])\n",
    "\n",
    "\n",
    "imputed_X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LOAN              0\n",
       "VALUE             0\n",
       "YOJ               0\n",
       "DEROG             0\n",
       "DELINQ            0\n",
       "CLAGE             0\n",
       "NINQ              0\n",
       "CLNO              0\n",
       "DEBTINC           0\n",
       "JOB_Mgr           0\n",
       "JOB_Office        0\n",
       "JOB_Other         0\n",
       "JOB_ProfExe       0\n",
       "JOB_Sales         0\n",
       "JOB_Self          0\n",
       "JOB_missing       0\n",
       "REASON_DebtCon    0\n",
       "REASON_HomeImp    0\n",
       "REASON_missing    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_X_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unskewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "unskewed_X_train = imputed_X_train.copy()\n",
    "unskewed_X_test = imputed_X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_cols = ['JOB_Mgr', 'JOB_Office', 'JOB_Other', 'JOB_ProfExe','JOB_Sales', 'JOB_Self', 'JOB_missing', 'REASON_DebtCon','REASON_HomeImp', 'REASON_missing',\"DEBTINC\", \"CLNO\", \"CLAGE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NINQ      8.431721\n",
       "DELINQ    5.158540\n",
       "DEROG     5.017884\n",
       "VALUE     4.171394\n",
       "LOAN      2.121965\n",
       "YOJ       1.040289\n",
       "dtype: float64"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unskewed_X_test.drop(na_cols, axis=1).skew().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEROG     5.821586\n",
       "NINQ      4.643368\n",
       "DELINQ    3.806487\n",
       "VALUE     2.736777\n",
       "LOAN      1.973491\n",
       "YOJ       1.031896\n",
       "dtype: float64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unskewed_X_train.drop(na_cols, axis=1).skew().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEROG     2.946998\n",
       "DELINQ    2.066387\n",
       "NINQ      0.757694\n",
       "VALUE    -0.085419\n",
       "LOAN     -0.157794\n",
       "YOJ      -0.708165\n",
       "dtype: float64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in unskewed_X_test.drop(na_cols, axis=1).columns:\n",
    "    unskewed_X_test[col] = np.log1p(unskewed_X_test[col])\n",
    "    unskewed_X_train[col] = np.log1p(unskewed_X_train[col])\n",
    "    \n",
    "unskewed_X_test.drop(na_cols, axis=1).skew().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEROG     3.053931\n",
       "DELINQ    2.038537\n",
       "NINQ      0.694626\n",
       "VALUE    -0.089862\n",
       "LOAN     -0.329866\n",
       "YOJ      -0.635137\n",
       "dtype: float64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unskewed_X_train.drop(na_cols, axis=1).skew().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to be working with distance based algorithms thus all feature scales need to be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = unskewed_X_train.copy()\n",
    "scaled_X_test = unskewed_X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(series):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(series.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_cols = ['JOB_Mgr', 'JOB_Office', 'JOB_Other', 'JOB_ProfExe','JOB_Sales', 'JOB_Self', 'JOB_missing', 'REASON_DebtCon','REASON_HomeImp', 'REASON_missing']\n",
    "\n",
    "for col in scaled_X_train.drop(na_cols, axis=1).columns:\n",
    "    scaled_X_train[col] = scale_data(scaled_X_train[col])\n",
    "    scaled_X_test[col] = scale_data(scaled_X_test[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable BAD is imbalanced. There are many approaches to deal with imbalanced data: class weights, solo classifier, undersampling, oversampling, hybrid methods ... . For this project I will use the SMOTE approach since it seems to do well on this type of project according to several papers on this topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "balanced_X_train, balanced_y_train = smote.fit_resample(unskewed_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BAD\n",
       "0    3834\n",
       "1     932\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BAD\n",
       "0    3834\n",
       "1    3834\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving cleaned datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_X_train.to_csv('./data/cleaned_X_train.csv')\n",
    "balanced_y_train.to_csv('./data/cleaned_y_train.csv')\n",
    "\n",
    "scaled_X_test.to_csv('./data/cleaned_X_test.csv')\n",
    "y_test.to_csv('./data/cleaned_y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
